#Project Overview

# This section provides a detailed overview of the project's foundational steps, from setting up the environment
# to performing initial data health checks. This groundwork is essential for ensuring the integrity and reliability
# of the subsequent analysis and modeling.#Project Overview


# Step 1: Install & Imports Summary

# A single confirmation message: ✅ All packages installed and ready to use!. This indicates a successful
# installation and import of all libraries, preparing the environment for the subsequent analysis and modeling tasks.

# Step 1: Install & Imports Summary

# This cell installs and imports all the necessary Python libraries for data analysis, visualization,
# and advanced machine learning models, including XGBoost and tools for hyperparameter tuning.

!pip install -q pandas numpy matplotlib seaborn scikit-learn statsmodels scipy openpyxl xgboost
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
from scipy import stats
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV # Added import for hyperparameter tuning
from scipy.stats import randint as sp_randint # Added import for RandomizedSearchCV
import xgboost as xgb # Added import for XGBoost

# Visual config

sns.set_theme(style="whitegrid")
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', '{:,.4f}'.format)

print("✅ All packages installed and ready to use!")

# Step 2: Load Data and Initial Overview

# This cell loads the cleaned CSV file into a pandas DataFrame and then prints a summary of the data, including column types
# and the first few rows.
# This serves as a critical first check to ensure the data has been loaded correctly and is in the expected format.

# Import pandas and sys

import pandas as pd
import sys


# Assume file_path is defined
file_path = "medical_costs_for_insurance.csv"

try:
    df = pd.read_csv(file_path)
    print("Data loaded successfully!")
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found. Please ensure the file is uploaded in Colab's file panel.")
    sys.exit()
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    sys.exit()

print("\n--- Data Info ---")
df.info()

print("\n--- First 5 Rows ---")
print(df.head())

# Step 2: Summary of DataFrame

# The output will first show which path was successfully used to load the data, for example:
# Loaded dataset from: /content/medical_costs_for_insurance.csv.
# This is followed by a preview of the first five rows of the DataFrame, confirming that the data has been correctly loaded into the notebook.

# Cell 3 - Quick data health checks

# This cell performs initial checks on the loaded data to understand its shape, types, and quality, including the presence of missing values and duplicates.

print("Shape (rows, cols):", df.shape)
print("\nColumn types:")
display(df.dtypes)
print("\nBasic descriptive stats:")
display(df.describe(include='all').T)
print("\nMissing values per column:")
display(df.isnull().sum())
print("\nDuplicate rows count:", df.duplicated().sum())


# Step 3: Summary of Quick data health checks

# The output provides a multi-part summary:

# Shape: A tuple showing the total number of rows and columns, e.g., (1338, 7).

# Column types: A list of all column names with their data types, such as object for strings and int64 for integers.

# Descriptive stats: A statistical summary table with measures like mean, standard deviation, and quartiles for numeric columns, and top values and frequencies for categorical columns.

# Missing values: A count of null values for each column. The expected output is a list of all columns with a count of 0.

# Duplicates: A count of the total number of duplicate rows found, which should be 1.

# Step 4: Normalize column names and create fallback engineered features if missing

# This cell ensures data consistency and creates several key features necessary for the machine learning models.

# remove accidental whitespace

df.columns = [c.strip() for c in df.columns]

# Standardize smoker column if present

if 'smoker' in df.columns:
    df['smoker'] = df['smoker'].astype(str).str.lower().map({'yes': 'yes', 'no': 'no'}).fillna(df['smoker'])

# Identify main charges column (support common name variants)
charges_candidates = ['total_charges','charges','totalcharge','total_charge','charge']
charges_col = None
for c in charges_candidates:
    if c in df.columns:
        charges_col = c
        break
if charges_col is None:
    raise KeyError("No charges column found. Expected one of: " + ", ".join(charges_candidates))

# Create combined_health_status if missing (example logic copied from Diddle)
if 'combined_health_status' not in df.columns:
    def health_status(row):
        try:
            bmi = float(row.get('bmi', np.nan))
            smoker = str(row.get('smoker', 'no')).lower()
        except:
            bmi = np.nan
            smoker = 'no'
        if smoker == 'yes' and bmi >= 30:
            return 'high_risk'
        elif bmi >= 25:
            return 'medium_risk'
        else:
            return 'low_risk'
    df['combined_health_status'] = df.apply(health_status, axis=1)

# Create smoker_binary if missing
if 'smoker_binary' not in df.columns:
    df['smoker_binary'] = df['smoker'].map({'yes': 1, 'no': 0}).fillna(0).astype(int)

# Create log_charges if missing (use clipped to avoid log(0) or negatives)
if 'log_charges' not in df.columns:
    df['log_charges'] = np.log(df[charges_col].clip(lower=1))

print("Engineered/fallback columns ensured: combined_health_status, smoker_binary, log_charges")


#Step 4: Normalizing Column Names Summary

# A single confirmation message: Engineered/fallback columns ensured: combined_health_status, smoker_binary, log_charges.
# This confirms that the dataset now contains all the necessary features, either from the original file or created by the script.

# Step 5:  Query 1 - Top N highest-charge records with context

# This cell sorts the data to quickly inspect the records with the highest medical charges.

top_n = 10
context_cols = ['age','sex','bmi','children','smoker','region','combined_health_status','bmi_category','bmi_health_status']
context_cols = [c for c in context_cols if c in df.columns]  # keep only those that exist
display(df.sort_values(by=charges_col, ascending=False).head(top_n)[[charges_col] + context_cols])


# Step 5: Summary of Query 1

# A table showing the top 10 records by total charges. The table includes columns like age, sex, bmi, smoker, and combined_health_status.
# The output will visually highlight that most, if not all, of the highest-charge records are associated with the 'yes' value in the smoker column.

# Step 6: Query 2 - Value counts for validation of new features

# This cell verifies that the newly created categorical and binary features have been populated correctly.

for col in ['combined_health_status', 'smoker_binary', 'bmi_category', 'bmi_health_status']:
    if col in df.columns:
        print(f"\nValue counts for {col}:")
        display(df[col].value_counts(dropna=False))
    else:
        print(f"\nColumn '{col}' not present in dataset.")


# Step 6: Summary of Query 2

# The output displays a list of value counts for each specified column. For smoker_binary, for example, the output will show the counts for 0 and 1.
# This confirms that the feature engineering was successful and the data is distributed as expected.

# Cell 7:  Query 3 - Smoking impact: descriptive & statistical test

# This cell calculates descriptive statistics for charges by smoking status and performs a statistical test to confirm the significance of the difference.

if 'smoker' in df.columns:
    grouped = df.groupby('smoker')[charges_col].agg(['count','mean','median','std']).rename(columns={'mean':'avg'})
    print("Charges by smoker status:")
    display(grouped)

    smokers = df[df['smoker_binary'] == 1][charges_col].dropna()
    nonsmokers = df[df['smoker_binary'] == 0][charges_col].dropna()
    if len(smokers) >= 2 and len(nonsmokers) >= 2:
        tstat, pval = stats.ttest_ind(smokers, nonsmokers, equal_var=False, nan_policy='omit')
        print(f"Welch t-test (smokers vs non-smokers) on {charges_col}: t = {tstat:.4f}, p = {pval:.4f}")
    else:
        print("Not enough data to run t-test.")
else:
    print("No 'smoker' column to analyze.")



# Step 7: Summary of Query 3

# A table showing the average, median, and count of charges for both smokers and non-smokers.
# The average charges for smokers are significantly higher than for non-smokers.
# A Welch's t-test result is also displayed, with a very small p-value (e.g., p = 0.0000), which indicates a highly statistically significant difference between the two groups.

# Step 8: Visuals for distributions and smoker effect

# This cell generates three plots to visualize the distribution of charges and the impact of smoking.

plt.figure(figsize=(10,4))
sns.histplot(df[charges_col].dropna(), kde=True)
plt.title("Distribution of charges")
plt.xlabel(charges_col)
plt.show()

plt.figure(figsize=(10,4))
sns.histplot(df['log_charges'].dropna(), kde=True)
plt.title("Distribution of log(charges)")
plt.xlabel("log_charges")
plt.show()

if 'smoker' in df.columns:
    plt.figure(figsize=(8,6))
    sns.boxplot(x='smoker', y=charges_col, data=df)
    plt.title("Charges by smoker status (boxplot)")
    plt.yscale('log') # use log scale to make skewed distribution readable
    plt.show()


# Step 8: Summary of Visuals for distributions and smoker effect


# Three plots are generated:

# A histogram of raw charges, showing a long right-skewed tail.

# A histogram of log-transformed charges, showing a more symmetric, bell-shaped distribution. This confirms the effectiveness of the log transformation.

# A boxplot of charges by smoker status (on a log scale), which visually shows the much higher median and wider spread of charges for smokers compared to non-smokers.

# Step 9:  Deep dive visualization: Charges vs. Age, colored by Smoker Status

# This plot visualizes the relationship between age and total_charges while also showing the effect of smoking.

# Create a figure and axis for the plot
plt.figure(figsize=(12, 8))

# Use seaborn's scatterplot to plot age vs. log_charges,
# with hue='smoker' to differentiate the groups
sns.scatterplot(x='age', y=charges_col, hue='smoker', data=df, alpha=0.7)

# Set the title and axis labels
plt.title("Medical Charges vs. Age, by Smoker Status", fontsize=16)
plt.xlabel("Age", fontsize=12)
plt.ylabel("Total Charges (Log Scale)", fontsize=12)

# Use a logarithmic scale on the y-axis to handle the wide range of charge values
plt.yscale('log')

# Add a legend
plt.legend(title='Smoker Status')

# Show the plot
plt.show()

# Step 9: Summary of Deep dive visualization

# A scatter plot with age on the x-axis and total_charges (on a log scale) on the y-axis.
# Data points are colored based on smoker status. The plot will show two distinct groups of data points, with the group of smokers having
# significantly higher charges across all age ranges, visually demonstrating smoking as the dominant factor.

# Step 10:  Numeric correlations & charge relationships

# This cell calculates and visualizes the linear relationships between all numerical features.


numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if charges_col in numeric_cols:
    corr_with_charges = df[numeric_cols].corr()[charges_col].sort_values(ascending=False)
    print("Numeric correlations with charges:")
    display(corr_with_charges)
else:
    print("Charges column not numeric; correlation skipped.")

plt.figure(figsize=(10,8))
sns.heatmap(df[numeric_cols].corr(), annot=False, cmap='vlag', center=0)
plt.title("Numeric correlation matrix")
plt.show()


# Step 10: Summary of Numeric correlations & charge relationships

# The output includes two parts:

# A list of all numerical columns sorted by their correlation with the charges column. smoker_binary is
# expected to have the highest correlation value, followed by age and bmi.

# A correlation heatmap, a color-coded matrix that visually represents the correlation between all numerical features.

# Step 11:  Regression model (OLS) to explain log_charges

# This cell fits a simple, interpretable linear regression model using statsmodels to predict log_charges.

base_features = ['age','bmi','children','smoker_binary']
region_binaries = [c for c in df.columns if c.startswith('region_') and c.endswith('_binary')]
features = [f for f in base_features + region_binaries if f in df.columns]

X = df[features].copy().fillna(0)
y = df['log_charges']

# Add constant for statsmodels
X_sm = sm.add_constant(X)
ols_model = sm.OLS(y, X_sm, missing='drop').fit()
print(ols_model.summary())



# Step 11: Summary of Regression model

# A detailed summary table of the OLS model's results. Key metrics to look for include:

# R-squared: A value around 0.75, indicating that the selected features explain about 75% of the variance in log_charges.

# Coefficients: The coefficients for age, bmi, and especially smoker_binary will be large and positive, indicating a strong positive relationship with log_charges.

# P-values: The p-values for the main features will be close to 0, confirming their statistical significance.

# Step 12: Quick train/test to get R2 and RMSE on log scale

# This cell performs a train-test split and evaluates a simple LinearRegression model to get a realistic measure of its performance on unseen data.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print("R2 (test):", r2_score(y_test, y_pred))
print("RMSE (test, log scale):", np.sqrt(mean_squared_error(y_test, y_pred)))



# Step 12: Summary of Qucik train/test

# The output will display two key metrics for the linear model's performance on the test data:

# R2 (test): A value close to the R-squared from the OLS summary, but a more reliable indicator of the model's predictive power.

# RMSE (test, log scale): A value representing the average error of the predictions, measured in log-charge units.

# Step 13: Adding an interaction term for Advanced Feature Engineering

# This cell creates and tests a new feature—an interaction term between age and smoker—and evaluates if it improves the linear model's performance.

# An interaction term can capture non-linear effects between variables
# Here we'll create an interaction between age and smoker status

if 'age' in X.columns and 'smoker_binary' in X.columns:
    X_with_interaction = X.copy()
    X_with_interaction['age_x_smoker'] = X_with_interaction['age'] * X_with_interaction['smoker_binary']
    print("New feature created: 'age_x_smoker'")
    print("Retraining the linear model with the new feature...")
    X_train_int, X_test_int, y_train_int, y_test_int = train_test_split(X_with_interaction, y, test_size=0.20, random_state=42)

    lr_int = LinearRegression()
    lr_int.fit(X_train_int, y_train_int)
    y_pred_int = lr_int.predict(X_test_int)

    print("Linear Regression with 'age_x_smoker' interaction term:")
    print("R2 (test):", r2_score(y_test_int, y_pred_int))
    print("RMSE (test, log scale):", np.sqrt(mean_squared_error(y_test_int, y_pred_int)))
else:
    print("Could not create interaction term 'age_x_smoker' as required columns are missing.")

# Step 13: Summary of Advanced Feature Engineering

# The output confirms the creation of a new feature and then displays the R-squared and RMSE for the new model.
# The R2 value should be slightly higher, and the RMSE slightly lower, than the previous linear model, indicating that the interaction term improved the model's accuracy.

# Step 14: Robustness Check using Bootstrapping

# The goal of this step is to check if the coefficients of the simple linear model are stable.
# A technique called bootstrapping, which involves running the same model on
# many different random samples of our data with replacement, will be utilized.
# If the coefficients for key features like 'smoker_binary' and 'age' remain consistent,
# there will be more confidence in the model's findings.

print("Performing robustness check on the linear model using bootstrapping...")

# Create a list to store the coefficients from each bootstrap sample

bootstrapped_coeffs = []
num_bootstraps = 100 # A reasonable number of bootstrap samples

# The features we want to check for stability

check_features = ['age', 'bmi', 'smoker_binary']
if not all(f in X.columns for f in check_features):

    # This might happen if previous steps failed to create these features

    print("Required features for robustness check are missing. Skipping.")
else:
    # Loop to create and train the model on each bootstrap sample

    for i in range(num_bootstraps):

        # Sample with replacement from the original data
        # The sample size is the same as the original DataFrame's length

        sample_df = df.sample(n=len(df), replace=True, random_state=i)

        # Prepare X and y for the current sample

        X_sample = sample_df[check_features].copy().fillna(0)
        y_sample = sample_df['log_charges']

        # Fit a new OLS model on the bootstrap sample

        X_sm_sample = sm.add_constant(X_sample)
        ols_model_sample = sm.OLS(y_sample, X_sm_sample, missing='drop').fit()

        # Store the coefficients, excluding the constant

        coeffs = ols_model_sample.params.drop('const')
        bootstrapped_coeffs.append(coeffs)

    # Convert the list of coefficients to a DataFrame for easy analysis

    coeffs_df = pd.DataFrame(bootstrapped_coeffs)

    # Calculate the mean and standard deviation of the coefficients

    mean_coeffs = coeffs_df.mean().rename('Mean Coefficient')
    std_coeffs = coeffs_df.std().rename('Std. Deviation')

    # Combine the results into a single table

    results = pd.concat([mean_coeffs, std_coeffs], axis=1)

    print("\nRobustness Check Results (Bootstrapped Coefficients):")
    display(results)

    # A simple check for stability
    # The coefficient is considered stable if the standard deviation is small relative to the mean

    print("\nInterpretation:")
    for feature in check_features:
        mean_val = results.loc[feature, 'Mean Coefficient']
        std_val = results.loc[feature, 'Std. Deviation']
        if std_val < 0.1 * abs(mean_val): # A heuristic check
            print(f"✅ The coefficient for '{feature}' appears stable (Mean: {mean_val:.4f}, Std: {std_val:.4f}).")
        else:
            print(f"⚠️ The coefficient for '{feature}' may be less stable (Mean: {mean_val:.4f}, Std: {std_val:.4f}).")


# Step 14: Summary of Robustness Check

# The output of this cell is a table that displays the mean coefficient and the standard deviation for each of the features being checked.

# Mean Coefficient: This value represents the average impact of a feature on log_charges across all the bootstrap samples.
#This value should be consistent with the coefficient found in the original OLS model in Step 11.

# Std. Deviation: This metric indicates the amount of variability in the coefficient across the different samples.
#A small standard deviation suggests the relationship is highly stable and reliable. A larger value would be a warning sign,
#indicating that the feature's influence might be sensitive to the specific data points in a sample.

# The output also includes a simple interpretation, providing a quick assessment of whether each feature's coefficient is considered stable.
#This confirms that the model's findings are dependable before proceeding with more complex models.

# Step 15: Advanced Model: Random Forest with Hyperparameter Tuning

# This cell introduces a more powerful ensemble model and uses a randomized search to find its optimal hyperparameters.

# This cell uses RandomizedSearchCV to find the best settings for the Random Forest model
# This is much more efficient than trying every combination

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint as sp_randint

# Import sp_randint

rf_model = RandomForestRegressor(random_state=42)

# Define the parameter space to search

param_dist = {
    'n_estimators': sp_randint(50, 200),
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': sp_randint(2, 11),
    'min_samples_leaf': sp_randint(1, 11)
}

# Use RandomizedSearchCV to find the best parameters

print("Starting Randomized Search for best Random Forest parameters...")
random_search = RandomizedSearchCV(rf_model, param_distributions=param_dist, n_iter=10, cv=3, random_state=42, n_jobs=-1, verbose=1)
random_search.fit(X_train, y_train)

# Get the best model and its performance

best_rf_model = random_search.best_estimator_
y_pred_tuned = best_rf_model.predict(X_test)
print("\nRandom Forest Model with Hyperparameter Tuning Performance:")
print("Best parameters found:", random_search.best_params_)
print(f"R-squared (test): {r2_score(y_test, y_pred_tuned):.4f}")
print(f"RMSE (test, log scale): {np.sqrt(mean_squared_error(y_test, y_pred_tuned)):.4f}")

# You can also get a feel for feature importance from the tuned model

feature_importances = pd.Series(best_rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print("\nFeature Importances from Tuned Model (Top 5):")
display(feature_importances.head())

# Step 15: Summary of Random Forest

# The output details the process of the randomized search and the final performance of the tuned model:

# A summary of the randomized search process.

# Best parameters found: The combination of n_estimators, max_depth, min_samples_split, and min_samples_leaf that resulted in the best model.

# R-squared (test) and RMSE (test, log scale): These values will be significantly better (higher R-squared, lower RMSE) than the linear models,
# showing that the Random Forest model captures more complex patterns.

# A table of Feature Importances, which will likely show smoker_binary as the most important feature by a large margin.

# Step 16: More Sophisticated Modeling: XGBoost

# This cell trains another state-of-the-art model, XGBoost, for comparison.

# XGBoost is a powerful gradient boosting model that often performs very well
print("Training an XGBoost model...")
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

print("\nXGBoost Model Performance:")
print(f"R-squared (test): {r2_score(y_test, y_pred_xgb):.4f}")
print(f"RMSE (test, log scale): {np.sqrt(mean_squared_error(y_test, y_pred_xgb)):.4f}")

# Step 16: Summary of More Sophiscated Modeling

# The output displays the performance metrics for the XGBoost model on the test data.
# The R-squared and RMSE values are expected to be very similar to or slightly better than the Random Forest model,
# confirming the strong performance of gradient boosting algorithms on this dataset.

# Step 17: Inspect Anomalies

# This cell inspects the highest-cost record and its peers to understand potential reasons for extreme charges.

top_record = df.sort_values(by=charges_col, ascending=False).iloc[0]
print("Top record:")
display(top_record)

# Find peers with same combined_health_status (if available)

if 'combined_health_status' in df.columns:
    peers = df[df['combined_health_status'] == top_record['combined_health_status']].sort_values(by=charges_col, ascending=False).head(10)
    print(f"Top peers with same combined_health_status ({top_record['combined_health_status']}):")
    display(peers[[charges_col,'age','bmi','children','smoker','region']])
else:
    print("No combined_health_status column to find peers.")

# Step 17: Summary of Inspect Anomalies

# A table showing the details of the single record with the highest charges.
# This is followed by a second table showing the top 10 records with the same combined_health_status.
# The output will likely show that the highest-cost patient is a smoker with a high BMI, and their peers with the same health status also have high charges.

# Step 18: Save cleaned/augmented dataset for later use

# This final cell saves the cleaned and augmented DataFrame to a new CSV file.

outname = "medical_costs_for_insurance_cleaned.csv"
df.to_csv(outname, index=False)
print(f"Saved cleaned file: {outname}")

# In Colab this will prompt a browser download
try:
    from google.colab import files
    files.download(outname)
except Exception:
    print("If files.download() failed, the file is saved in the notebook working directory.")


# Step 18: Summary of Saving Dataset

# A confirmation message: Saved cleaned file: medical_costs_for_insurance_cleaned.csv.
# In a Google Colab environment, a browser download of the file will also be initiated.
# This step ensures that the final processed data is saved for future use or as a deliverable for the project.
